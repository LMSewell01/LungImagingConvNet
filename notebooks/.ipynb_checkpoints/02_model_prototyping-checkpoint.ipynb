{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1026f47a",
   "metadata": {},
   "source": [
    "# Lung Abnormality Multi-Class Segmentation: Iteration 1 Analysis\n",
    "This notebook documents the initial setup, training, and analysis of a deep learning model for multi-class segmentation of lung abnormalities from chest X-ray images. Our goal is to iteratively improve the model's ability to distinguish between healthy lung tissue and various types of infections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b812f0",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45435a7",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1. Introduction to the Problem\n",
    "Lung segmentation in chest X-rays is a critical task in medical imaging. It involves identifying and delineating specific regions of interest, such as healthy lung tissue and various types of abnormalities. This project focuses on multi-class semantic segmentation, aiming to classify each pixel into one of four categories:\n",
    "\n",
    "- Class 0: Background (pixels outside the lung or image boundary)\n",
    "\n",
    "- Class 1: Healthy Lung Tissue (pixels inside the lung boundary that are not identified as an infection)\n",
    "\n",
    "- Class 2: COVID-19 Infection (pixel regions specifically identified as COVID-19 related opacities)\n",
    "\n",
    "- Class 3: Non-COVID Infection (pixel regions within the lung boundary representing other infections like bacterial or viral pneumonia)\n",
    "\n",
    "Accurate segmentation can significantly aid clinicians in faster diagnosis, monitoring disease progression, and treatment planning, especially for respiratory illnesses like pneumonia and COVID-19.\n",
    "\n",
    "This project was undertaken with the primary motivation of practically implementing a 2D Convolutional Neural Network (CNN) for a real-world problem, following completion of Andrew Ng's DeepLearning.Ai course. It serves as a hands-on application of learned theoretical concepts in a challenging and impactful domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f08f1",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 The Chosen Dataset: COVID-QU-Ex\n",
    "For this project, we are using the COVID-QU-Ex Dataset.\n",
    "\n",
    "Why we chose it: This dataset is highly suitable for our multi-class segmentation goal because it provides a diverse collection of chest X-ray images categorized into COVID-19, Non-COVID (other pneumonias), and Normal cases. Crucially, it includes pixel-level annotations (masks) for both general lung boundaries and specific COVID-19 infection regions. The inclusion of Non-COVID cases is vital as it allows us to build a more robust, multi-class model that distinguishes between different pathologies, moving beyond simple binary \"abnormal vs. normal\" classification. This directly supports our objective of developing a more clinically useful and nuanced diagnostic tool.\n",
    "\n",
    "The decision to use this dataset was also heavily influenced by its ease of access and availability. Initial research into suitable medical imaging datasets quickly revealed that Kaggle datasets were the most accessible options for practical implementation. Other, more clinically recent and specialized datasets often understandably required extensive certifications or had paid access models, which we aimed to minimize for this project. Choosing an readily available dataset allowed us to focus our resources on the deep learning implementation and iterative model improvement rather than on complex data acquisition procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d448fe",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Brief Explanation of Convolutional Neural Networks (CNNs)\n",
    "Convolutional Neural Networks (CNNs) are a specialized class of deep learning models explicitly designed to process data with a known grid-like topology, such as images. They are the driving force behind most advancements in computer vision.\n",
    "\n",
    "At their core, CNNs use convolutional layers to automatically learn spatial hierarchies of features directly from raw image data. Instead of us having to manually design features (like edge detectors or corner detectors), a CNN learns these filters (small matrices) that detect relevant patterns at various levels of abstraction.\n",
    "\n",
    "> Convolutional Layer: This is the primary building block. It applies a set of learnable filters (also called kernels) to the input image. Each filter slides (convolves) across the image, performing element-wise multiplication and summing the results, producing a feature map that highlights specific patterns (e.g., edges, textures).\n",
    "\n",
    "> Pooling Layer (e.g., Max Pooling): These layers reduce the spatial dimensions (width and height) of the feature maps. This serves two main purposes: it reduces the computational cost of the network, and it makes the detected features more robust to slight variations in the position of the patterns in the input image.\n",
    "\n",
    "> Activation Functions (e.g., ReLU - Rectified Linear Unit): Applied after convolutional layers, these introduce non-linearity into the network. This allows the CNN to learn more complex and non-linear relationships within the data, which is essential for understanding intricate image patterns.\n",
    "\n",
    "For tasks like semantic segmentation, where we need a pixel-level output (i.e., classifying every pixel in the image), CNNs are typically structured in an \"encoder-decoder\" fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c4300",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Our Chosen Model: U-Net with ResNet50 Backbone\n",
    "Our model is a powerful combination: a U-Net architecture that leverages a ResNet50 pre-trained backbone. This blend is highly effective for medical image segmentation tasks.\n",
    "\n",
    "### 3.1 U-Net Architecture\n",
    "The U-Net architecture is famously named for its distinctive U-shape, comprising two main symmetrical parts:\n",
    "\n",
    "> Encoder (Contracting Path): This side functions like a traditional CNN. It progressively applies convolutional and pooling layers, gradually reducing the spatial dimensions of the input image while increasing the number of feature channels. Its role is to capture contextual information and learn robust features from the image (i.e., understanding what is in the image).\n",
    "\n",
    "> Decoder (Expansive Path): This side is responsible for precisely localizing the segmentation. It uses upsampling layers (like transposed convolutions or simple upsampling followed by convolutions) to gradually increase the spatial resolution of the feature maps. Crucially, it combines these upsampled features with high-resolution features directly from the encoder via skip connections.\n",
    "\n",
    "> Why U-Net for Segmentation: The U-Net's primary strength is its skip connections. These connections directly transfer fine-grained spatial information (details about edges, textures, and locations) from earlier layers of the encoder to the corresponding layers in the decoder. This prevents the loss of crucial boundary information that would otherwise occur during the encoder's downsampling process, enabling the decoder to produce highly accurate and localized segmentation masks.\n",
    "\n",
    "### 3.2 ResNet50 Backbone\n",
    "Instead of building a simple convolutional encoder from scratch, we enhance the encoder part of our U-Net by incorporating ResNet50 as its backbone.\n",
    "\n",
    "> ResNet50: A very deep Convolutional Neural Network, part of the Residual Network family. Its key innovation lies in its \"residual connections\" (also known as skip connections within the encoder blocks). These connections allow the gradients to flow more easily through many layers, mitigating the \"vanishing gradient\" problem and enabling the effective training of very deep networks.\n",
    "\n",
    "> Pre-trained on ImageNet: Our ResNet50 backbone is initialized with weights learned from the vast ImageNet dataset, a collection of millions of images across 1000 object categories. This technique is known as transfer learning.\n",
    "\n",
    "#### Why ResNet50 Backbone:\n",
    "\n",
    "> Feature Richness (Transfer Learning): By using a pre-trained ResNet50, our model starts with a strong foundation of learned visual features (edges, textures, shapes) that are generally applicable to many image tasks. This significantly reduces the amount of data and time needed to train the model, especially on smaller, specialized datasets like medical images.\n",
    "\n",
    "> Robust Feature Extraction: ResNet50 is known for its ability to extract powerful and hierarchical features, which serves as an excellent starting point for the U-Net's segmentation task.\n",
    "\n",
    "> Deep Network Capabilities: Its residual connections allow for deeper feature extraction, which can be beneficial for complex tasks like distinguishing subtle lung pathologies.\n",
    "\n",
    "By combining the U-Net's precise localization with ResNet50's powerful pre-trained feature extraction, we aim to build a robust and accurate multi-class lung segmentation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d497fcc",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4. Explanation of Scripts in src/\n",
    "\n",
    "Our project is organized into several Python scripts within the src/ directory, each with a specific responsibility:\n",
    "\n",
    "### src/main.py:\n",
    "\n",
    "Purpose: This is the main entry point for our entire project. It acts as a command-line interface.\n",
    "\n",
    "Functionality: It uses the argparse module to accept command-line arguments (e.g., --train, --evaluate, --predict). Based on the argument provided, it imports and calls the main function from the corresponding script (train.py, evaluate.py, or predict_and_visualize.py).\n",
    "\n",
    "Why: Provides a clean, unified way to run different tasks of the project without directly executing individual scripts.\n",
    "\n",
    "### src/data_loader.py:\n",
    "\n",
    "Purpose: Handles all aspects of loading, preprocessing, and augmenting our image and mask data.\n",
    "\n",
    "Functionality:\n",
    "\n",
    "get_covid_qu_ex_paths(base_data_dir): Scans the dataset directory, collects paths for images (Normal, COVID, Non-COVID), specific COVID-19 infection masks, and general lung masks. It intelligently constructs multi-class masks (0: Background, 1: Healthy, 2: COVID, 3: Non-COVID) based on the image type and mask availability. It also performs an 80/10/10 train/val/test split, stratified by image type.\n",
    "\n",
    "load_image_and_multi_class_mask(...): Reads individual image and mask files, resizes them to a consistent IMG_HEIGHT x IMG_WIDTH, normalizes pixel values, and converts the integer-encoded masks into one-hot encoded multi-class masks (e.g., a pixel with COVID-19 infection might become [0,0,1,0]).\n",
    "\n",
    "augment_data(image, mask): Applies basic data augmentation (e.g., horizontal flipping) consistently to both images and their corresponding masks.\n",
    "\n",
    "get_dataset(...): Creates optimized tf.data.Dataset objects, which are highly efficient for feeding data to TensorFlow models. It incorporates shuffling, mapping of loading/augmentation functions (num_parallel_calls=tf.data.AUTOTUNE), batching, and prefetching (buffer_size=tf.data.AUTOTUNE) for performance.\n",
    "\n",
    "Why: Centralizes data handling, ensures consistency in preprocessing, and creates an efficient data pipeline to prevent bottlenecks during GPU training.\n",
    "\n",
    "### src/model.py:\n",
    "\n",
    "Purpose: Defines the U-Net model architecture with a ResNet50 backbone.\n",
    "\n",
    "Functionality:\n",
    "\n",
    "build_unet_resnet50(input_shape, num_classes): Initializes the ResNet50 model (pre-trained on ImageNet) as the encoder. Crucially, it sets base_model.trainable = False to freeze the ResNet50 layers, allowing us to leverage transfer learning without training the entire massive network from scratch. It then constructs the decoder path, using skip connections from various ResNet50 encoder layers, and ends with a final convolutional layer that has NUM_CLASSES (4) output filters and a softmax activation function for multi-class pixel classification.\n",
    "\n",
    "Why: Separates the model definition from the training logic, making the code modular, reusable, and easier to modify. Freezing the backbone drastically reduces trainable parameters and training time.\n",
    "\n",
    "### src/train.py:\n",
    "\n",
    "Purpose: Manages the training process of the model.\n",
    "\n",
    "Functionality:\n",
    "\n",
    "Sets hyperparameters (BATCH_SIZE, EPOCHS, LEARNING_RATE).\n",
    "\n",
    "Enables Mixed Precision Training (tf.keras.mixed_precision.set_global_policy('mixed_float16')) and XLA JIT Compiler (tf.config.optimizer.set_jit(True)) for significant GPU acceleration.\n",
    "\n",
    "Defines custom multi-class loss (combined_loss_multi_class) and metric (dice_coeff_multi_class) functions using a combination of Categorical Cross-Entropy and Dice Loss.\n",
    "\n",
    "Compiles the build_unet_resnet50 model with the Adam optimizer and the custom loss/metrics.\n",
    "\n",
    "Sets up callbacks for training: ModelCheckpoint (to save the best model based on validation loss), EarlyStopping (to stop training if validation loss doesn't improve for several epochs), ReduceLROnPlateau (to reduce learning rate if validation loss plateaus), and TensorBoard (for logging training progress).\n",
    "\n",
    "Executes model.fit() to start the training loop.\n",
    "\n",
    "Why: Encapsulates the entire training procedure, including optimizations and monitoring tools, ensuring consistency and reproducibility of experiments.\n",
    "\n",
    "### src/evaluate.py:\n",
    "\n",
    "**Purpose**: Evaluates the performance of the trained model on the unseen test set.\n",
    "\n",
    "**Functionality**: Loads the trained model (best_unet_resnet50_multi_class.h5) from saved_models/, loads the test dataset via data_loader.py, and then calls model.evaluate() to compute the final loss and metrics (Categorical Accuracy, Dice Coefficient) on the test data.\n",
    "\n",
    "**Why**: Provides an unbiased assessment of the model's generalization capabilities on data it has never seen before.\n",
    "\n",
    "### src/predict_and_visualize.py:\n",
    "\n",
    "**Purpose**: Loads the trained model, makes predictions on test images, and visualizes the results.\n",
    "\n",
    "**Functionality**: Loads the best model, selects a few random images from the test set, performs inference (prediction) to generate multi-class segmentation masks, and then uses Matplotlib to display the original image, the true ground-truth mask, and the model's predicted mask side-by-side. It uses a custom COLORMAP and legend to clearly differentiate between the four output classes.\n",
    "\n",
    "**Why**: Provides crucial qualitative feedback on the model's performance. Visual inspection helps understand the types of errors the model makes and guides further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dbb0d8",
   "metadata": {},
   "source": [
    "5. Training Process on Google Colab: Optimizations and Setup\n",
    "Our initial training run was executed on Google Colab, leveraging its GPU resources for acceleration. The setup involved a series of steps to ensure optimal performance.\n",
    "5.2 GPU Optimizations Applied\n",
    "Before starting training, we implemented several key optimizations in src/train.py to maximize GPU utilization and reduce training time:\n",
    "\n",
    "Freezing ResNet50 Backbone: In src/model.py, we set base_model.trainable = False immediately after loading the pre-trained ResNet50.\n",
    "\n",
    "Why: This prevents the vast majority of the ResNet50's parameters (over 23 million) from being updated during training. Instead of learning general features from ImageNet again, the model focuses its learning on the relatively smaller U-Net decoder, which is responsible for task-specific segmentation. This drastically reduces the computational load per step.\n",
    "\n",
    "Mixed Precision Training: Enabled using tf.keras.mixed_precision.set_global_policy('mixed_float16') at the start of train.py's main function.\n",
    "\n",
    "Why: Modern GPUs (like the A100) are highly efficient at performing calculations using float16 (half-precision) data types, especially via their Tensor Cores. Mixed precision performs most operations in float16 while strategically keeping critical parts (like loss calculations) in float32 for numerical stability. This typically doubles training speed and reduces GPU memory usage.\n",
    "\n",
    "XLA (Accelerated Linear Algebra): Enabled using tf.config.optimizer.set_jit(True) before model compilation in train.py.\n",
    "\n",
    "Why: XLA is a compiler that optimizes TensorFlow graphs by fusing operations and generating highly efficient machine code tailored to the specific hardware (the A100 GPU). This can provide an additional boost in training speed.\n",
    "\n",
    "Optimized tf.data Pipeline: Our data_loader.py uses tf.data.AUTOTUNE for parallel mapping and prefetching (buffer_size=tf.data.AUTOTUNE).\n",
    "\n",
    "Why: This ensures that the CPU (or other processing units) prepares and loads the next batch of data in parallel while the GPU is busy training on the current batch. This prevents the GPU from sitting idle, waiting for data, which is a common bottleneck.\n",
    "\n",
    "Increased Batch Size to 128: Leveraging the A100 GPU's ample VRAM (40GB/80GB), we were able to significantly increase the BATCH_SIZE to 128.\n",
    "\n",
    "Why: Processing more samples in a single step allows for greater parallelization on the GPU. This means the GPU's many processing units can be kept busy simultaneously, leading to much faster training throughput per step. This high degree of parallelization through larger batch sizes was a key factor in achieving significantly faster training through the epochs, drastically reducing the total time required for the entire training run compared to smaller batch sizes on less powerful GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e92ea",
   "metadata": {},
   "source": [
    "6. Analyzing the Training Process\n",
    "After the training run completed on Google Colab, the generated TensorBoard logs were downloaded locally. In this section, we'll analyze the training process directly within this Jupyter notebook by loading these logs and visualizing the key metrics.\n",
    "\n",
    "6.1 Loading Training History from TensorBoard Logs\n",
    "TensorBoard saves training metrics (loss, accuracy, custom metrics) as event files. We can use tensorboard.backend.event_processing.event_accumulator to read these files and extract the scalar data, which can then be converted into a Pandas DataFrame for easier manipulation and plotting.\n",
    "\n",
    "First, ensure you have the necessary libraries installed (tensorflow for event_accumulator, pandas for DataFrame operations, matplotlib for plotting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dad0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leese\\Documents\\Projects\n",
      "/c/Users/leese/Documents/Projects\n",
      "Requirement already satisfied: tensorboard in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (6.31.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\leese\\documents\\projects\\lung_abnormality_segmentation\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-33e70c93ac90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_processing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent_accumulator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEventAccumulator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Define the base path to your TensorBoard logs directory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!pwd\n",
    "!pip install tensorboard\n",
    "\n",
    "# In a new code cell in your Jupyter Notebook\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# Define the base path to your TensorBoard logs directory.\n",
    "# IMPORTANT: This path now directly points to the 'fit_multi_class_Iter_1' folder\n",
    "# you confirmed TensorBoard is reading.\n",
    "LOG_DIR = '/logs/fit_multi_class_Iter_1/'\n",
    "\n",
    "# Find all run directories within the LOG_DIR (e.g., 'YYYYMMDD-HHMMSS')\n",
    "run_dirs = [os.path.join(LOG_DIR, d) for d in os.listdir(LOG_DIR) if os.path.isdir(os.path.join(LOG_DIR, d))]\n",
    "\n",
    "if not run_dirs:\n",
    "    print(f\"Error: No TensorBoard run directories found in {LOG_DIR}.\")\n",
    "    print(\"Please ensure your 'logs/fit_multi_class_Iter_1/' folder contains timestamped subdirectories (e.g., '20250821-100329').\")\n",
    "    print(\"Expected log structure: project_root/logs/fit_multi_class_Iter_1/YYYYMMDD-HHMMSS/train/events.out...\")\n",
    "    print(\"Expected log structure: project_root/logs/fit_multi_class_Iter_1/YYYYMMDD-HHMMSS/validation/events.out...\")\n",
    "    history_df = pd.DataFrame() # Create an empty DataFrame to prevent errors in next cell\n",
    "else:\n",
    "    # Sort to get the latest run directory (or choose a specific run if you have multiple)\n",
    "    latest_run_dir = sorted(run_dirs)[-1]\n",
    "    print(f\"Analyzing logs from run: {latest_run_dir}\")\n",
    "\n",
    "    # Paths to the 'train' and 'validation' event files within this run\n",
    "    train_log_dir = os.path.join(latest_run_dir, 'train')\n",
    "    val_log_dir = os.path.join(latest_run_dir, 'validation')\n",
    "\n",
    "    train_history_data = {}\n",
    "    val_history_data = {}\n",
    "    \n",
    "    # Track all tags found\n",
    "    all_found_tags = set()\n",
    "\n",
    "    # Load training events\n",
    "    if os.path.exists(train_log_dir):\n",
    "        event_acc_train = EventAccumulator(train_log_dir)\n",
    "        event_acc_train.Reload()\n",
    "        train_tags = event_acc_train.Tags()['scalars']\n",
    "        print(f\"Found **TRAIN** scalar tags: {train_tags}\")\n",
    "        all_found_tags.update(train_tags)\n",
    "        if not train_tags:\n",
    "            print(f\"Warning: No scalar tags found in train logs at {train_log_dir}. This could mean the log file is empty or corrupted.\")\n",
    "        for tag in train_tags:\n",
    "            train_history_data[tag] = [event.value for event in event_acc_train.Scalars(tag)]\n",
    "    else:\n",
    "        print(f\"Warning: Train log directory not found at {train_log_dir}. Skipping train metrics.\")\n",
    "\n",
    "    # Load validation events\n",
    "    if os.path.exists(val_log_dir):\n",
    "        event_acc_val = EventAccumulator(val_log_dir)\n",
    "        event_acc_val.Reload()\n",
    "        val_tags = event_acc_val.Tags()['scalars']\n",
    "        print(f\"Found **VALIDATION** scalar tags: {val_tags}\")\n",
    "        all_found_tags.update(val_tags)\n",
    "        if not val_tags:\n",
    "            print(f\"Warning: No scalar tags found in validation logs at {val_log_dir}. This could mean the log file is empty or corrupted.\")\n",
    "        for tag in val_tags:\n",
    "            val_history_data[tag] = [event.value for event in event_acc_val.Scalars(tag)]\n",
    "    else:\n",
    "        print(f\"Warning: Validation log directory not found at {val_log_dir}. Skipping validation metrics.\")\n",
    "\n",
    "    print(f\"\\nAll unique scalar tags found across train and validation: {list(all_found_tags)}\")\n",
    "\n",
    "    # Combine training and validation history into a single DataFrame\n",
    "    combined_history = {}\n",
    "    \n",
    "    # Directly add training metrics\n",
    "    for tag, values in train_history_data.items():\n",
    "        combined_history[tag] = values\n",
    "\n",
    "    # Directly add validation metrics (they should already have 'val_' prefix from TensorBoard logging)\n",
    "    for tag, values in val_history_data.items():\n",
    "        combined_history[tag] = values\n",
    "\n",
    "    history_df = pd.DataFrame(combined_history).fillna(method='ffill')\n",
    "    \n",
    "    if history_df.empty:\n",
    "        print(\"Warning: Combined history DataFrame is empty. This means no scalar data was successfully loaded from event files.\")\n",
    "        print(\"Please check the 'Found train scalar tags:' and 'Found validation scalar tags:' outputs above to confirm metrics were logged.\")\n",
    "    else:\n",
    "        print(\"\\nTraining History DataFrame Head:\")\n",
    "        print(history_df.head())\n",
    "        print(f\"\\nTraining History DataFrame Shape: {history_df.shape}\")\n",
    "\n",
    "    # Optional: You can save this DataFrame for later use\n",
    "    # history_df.to_csv('training_history_iteration1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e167c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49563a44",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
